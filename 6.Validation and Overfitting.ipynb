{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If there is a competition that have not proper public dan private data split?\n<t> or they have too little data, either in public or private leaderboard.\n    \n#### Select the most proper validation !"},{"metadata":{},"cell_type":"markdown","source":"## Validation Strategies"},{"metadata":{},"cell_type":"markdown","source":"### Validation Types\n1. Holdout\n2. K-fold\n3. Leave-one-out\n\n\n### Stratification Cases\nJust the way to ensure that we have similar target distribution over different folds.\n<t> Stratification is useful for:\n    - Small datasets\n    - Unbalanced datasets\n    - Multiclass classification"},{"metadata":{},"cell_type":"markdown","source":"## Data splitting strategies\nSet up validation to mimic train test split in competition !\n\n\n#### Importan outcome\nDifferent splitting strategies can differ significantly\n1. in generated features\n2. in a way the model will rely on that features\n3. in some kind of target leak\n\nHave to identify train test split made by organizers, then reproduce it in train validation!"},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into train and validation\n1. Random, rowwise -->Every row are independent each other\n\n<t> But maybe there is some dependency in each row, for example :\n    - Between family member or people which work in the same company. If the husband can buy the credit, then probably the wife can buy the credit too. If the husband object are  in the train set and the wife are in the test set, we can generate special feature to handle this case.\n    \n    \n2. Timewise --> Special approach to feature generation based on the target !\n\n<t> Example: Predict the number of customer for each day in the next week, we can came up with the number of customer with the same day in previous week or the average number for the past month.\n \n#### Moving window\n\n    \n3. By id\n\n<t> Example a test for music recommendation for completely a new user. This means, we have different set of user in train and test.\n<t> We can make a conclusion, that features based on user history like how many user listen song in last week will not help for completely new user\n    \n<t> Picture that can take one after another, it could make bias, we need to random it !\n    \n    \n4. Combined\n\n<t> The train test split by single date, but the public private split was made by different dates for different geographic area\n    \n    \n    \n### Note :\n- Your validation should always mimic train test split made by organizers.\n\n<t> Example: The test set contains completely new search terms ! Cannot mimic the traintest split use random split or a search term split for validation\n<t> In order to mimic the model, it's crucial to mimic the ratio of new search term from test, traintest split.\n\n- Logic of feature generation depends on the data splitting strategy.  \n"},{"metadata":{},"cell_type":"markdown","source":"## Problems occuring during Validation\nUsually caused by inconsistentcy data, can be shown by getting different optimal parameters for different folds.\n1. Validation stage --> Make thorough validation !\n2. Submission stage --> Score validation and on the leaderboard don't match, caused by we can't mimic the train test split on our validation"},{"metadata":{},"cell_type":"markdown","source":"### Validation Stage\nCauses of different scores and optimal parameters:\n1. Too little data\n2. Too diverse and inconsistent data\n\n<t> We should do extensive validation\n \n 1. Average scores from different KFold splits --> make k-fold several times with different random seeds.\n 2. Tune model on one split, evaluate score on the other --> One check model paramaters and the other kfold check the model quality !\n \n \n\n#### This is useful when the score are closely each other, use thorough validation to be the top in competition !"},{"metadata":{},"cell_type":"markdown","source":"### Submission Stage\nWe can observe that:\n- LB score is consistently higher/lower that validation score.\n- LB score is not correlated with validation score at all.\n\n<t> Use EDA to find the root of your problem !"},{"metadata":{},"cell_type":"markdown","source":"We may already have quite different scores in kfold\n<t> Other reasons for problems:\n    1. too little data in public leaderboard\n    2. train and test data are from different distributions"},{"metadata":{},"cell_type":"markdown","source":"### Submission Stage: different distributions\nOur model will be have terrible score in our test data, because different distributions between train and test data.\n<t> How to handle it ?\n    - Solved by adjusting solution during the training procedure\n    - Solved only by adjusting solution to the leaderboard, this is called leaderboard probing --> To find the optimal constant, then shifted the value to the test data !\n    \n    \n<t> Another Example: If train set is consist mostly women, but in the test set is consist mostly men. How to overcome this ? Force your validation to mimic the test set !\n    \n    \n### Summary\n- too little data in public leaderboard\n- incorrect train/test split\n- different distributions in train and test"},{"metadata":{},"cell_type":"markdown","source":"## Leaderboard Shuffle\nExpect LB  shuffle because of :\n1. Randomness --> All participant have very close score !\n2. Little amount of data\n3. Different public/private distributions"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n- If we have big dispersion of scores on validation stage, we should do extensive validation.\n  1. Average scores from different KFold splits\n  2. Tune model on one split, evaluate score on the other\n  \n- If submission's score do not match local validation score, we should\n  1. Check if we have too little data in public LB\n  2. Check if we overfitted\n  3. Check if we chose correct splitting strategy\n  4. Check if train/test have different distributions\n  \n- Expect LB shuffle because of\n  1. Randomness\n  2. Little amount of data\n  3. Different public/private distributions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}