{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using target to generate features"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"rows = [['Moscow', 1,0.4,0],['Moscow', 1,0.4,1],['Moscow', 1,0.4,1],['Moscow', 1,0.4,0],['Moscow', 1,0.4,0],\n       ['Tver', 2,0.8,1],['Tver', 2,0.8,1],['Tver', 2,0.8,1],['Tver', 2,0.8,0],\n       ['Klin', 0,0,0],['Klin',0,0,0],['Tver', 2,0.8,1]]\n\n\ndf = pd.DataFrame(rows, columns = ['feature','feature_label','feature_mean','target'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feature_label are label encodings\n<t> feature_mean are mean encodings\n    \n ## Why does it work ?\n \n 1. Label encoding gives random order. No correlation with target\n 2. Mean encoding helps to separate zeros from ones "},{"metadata":{},"cell_type":"markdown","source":"In general, the more complex a non-linear target dependencies, the more effective mean encodings\n<t> There are countless possibilities to derived feature from target."},{"metadata":{},"cell_type":"markdown","source":"### Indicator of usefulness for mean encodings\nThe presence of categorical variables with a lot of level is already a good indicator\n\n<t> Use xgboost, then compare the depth for 7,9,11 quality. If we can see the more depth it has the more good score we had. This is true for train set, but for the test set ? if we didn't get overfit,that is weird. This tells us something, that some features had tremendous amount of split to get good prediction.\n    \n<t> Can help our model with mean encodings !"},{"metadata":{},"cell_type":"markdown","source":"## Ways to use target variable\nGoods - number of ones in a group\n<t> Bads - number of zeros\n    \n- Likelihood = $\\frac{Goods}{Goods+Bads} = mean(target)$\n- Weight of Evidence = $ln \\frac{Goods}{Bads} \\times 100$\n- Count = Goods = sum(target)\n- Diff = Goods - Bads\n\nExample code :\n```python\nmeans = X_tr.groupby(col).target.mean()\ntrain_new[col+'_mean_target'] = train_new[col].map(means)\nval_new[col+'_mean_target'] = val_new[col].map(means)\n\ndtrain = xgb.DMatrix(train_new, label=y_tr)\ndvalid = xgb.DMatrix(val_new, label=y_val)\n\nevallist = [(dtrain,'train'), (dvalid,'eval')]\nevals_result3 = {}\nmodel = xgb.train(xgb_par, dtrain, 3000, evals = evallist,verbose_eval=30, evals_result=evals_result3,early_stopping_rounds=50)\n```\n\nIf this cause overfit, we need to overcome with regularization !"},{"metadata":{},"cell_type":"markdown","source":"## Regularization\n1. CV loop inside training data\n    - Robust and intuitive\n    - Usually decent results with 4-5 folds across different datasets\n    - Set the mean encodings inside the cv loop\n    - Need to be careful with extreme situations like LOO\n    \n   ```python\n    y_tr = df_tr['target'].values # target variable\n    skf = StratifiedKFold(y_tr,5, shuffle=True, random_state=123)\n    \n    for tr_ind, val_ind in skf:\n        X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]\n        for col in cols : #iterate though the columns we want to encode\n            means = X_val[col].map(X_tr.groupby(col).target.mean())\n            X_val[col+'_mean_target'] = means\n        train_new.iloc[val_ind] = X_val\n        \n    prior = df_tr['target'].mean() #global mean\n    train_new.fillna(prior,inplace=True) #fill NANs with global mean```\n    \n    - Perfect feature for LOO scheme\n    - Target variable leakage is still present even for KFold scheme\n    \n2. Smoothing Regularization\n    - Alpha controls the amount of regularization. If categories is big, means has a lot of data points, can trust the mean encodings. But if it is very rare, the opposite.\n    \n    <t> Formula : $\\frac{mean(target) * nrows + globalmean * alpha}{nrows+alpha}$, alpha is equal to categorize size we can trust !\n    <t> Possible to use another formula, anything that punishes of rare categories can be considered smoothing\n    \n    \n3. Noise \n    - Noise degrades the quality of encoding\n    - How much noise should we add ? --> hard to make, it is not stable \n    - Usually used together with LOO\n    \n    \n4. Expanding Mean\n    - Least amount of leakage\n    - No hyperparameters\n    - Irregular encoding quality\n    - Built - in  in catboost --> Magnificent on a dataset categorical features\n    \n    ```python\n    cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']\n    cumcnt = df_tr.groupby(col).cumcount()\n    train_new[col+'_mean_target'] = cumsum/cumcnt ```"},{"metadata":{},"cell_type":"markdown","source":"## Extension and Generalization\n1. Regression and multiclass\n    - More statistics for regression tasks. Percentiles, std, distribution bins.\n    - Introducing new information for one vs all classifiers in multi class tasks. \n    \n2. Many-to-many relations\n    - Long representation\n    - Statistics from vectors\n    \n    \n3. Time Series\n    - Time structure allows us to make a lot of complicated features\n    - Rolling statistics of target variable\n    - The more data we have, the more complex feature we can create\n    \n    \n4. Interactions and numerical features\n    - Binning numeric features and treat it as a categorical feature\n    - Train the model with xgboost without any encodings. If numeric feature has a lot of split point it means that it has some kind complicated dependency to target --> Worth trying to mean encoded that feature !\n    - Then this exact point split feature, can be use to bin the numeric feature!\n    - How to select useful combination features ?\n    - How to extract interaction feature from decision tree?\n    - 2 features are interact in a tree, if they are in two neighbouring nodes. So, we can calculate how many times each feature interactions appear.\n    - The most frequent interactions are more probably worthy to mean encodings. Then, we can concatenate those 2 features and mean encodings it !\n    - Catboost model is correlated to feature interaction --> Good for a lot of categorical variable dataset"},{"metadata":{},"cell_type":"markdown","source":"### Correct validation reminder\n1. Local experiments:\n    - Estimate encodings on X_tr\n    - Map them to X_tr and X_val\n    - Regularize on X_tr\n    - Validate model on X_tr/X_val split\n    \n2. Submission:\n    - Estimate encodings on whole Train data\n    - Map them to Train and Test\n    - Regularize on Train\n    - Fit on Train\n    \n    \n### Summary\n1. Main advantages:\n    - Compact transformation of categorical variables\n    - Powerful basis for feature engineering\n   \n2. Disadvantages:\n    - Need careful validation, there a lot of ways to overfit\n    - Significant improvements only on specific datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}