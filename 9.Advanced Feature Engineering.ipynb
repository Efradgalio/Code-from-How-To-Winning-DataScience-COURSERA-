{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistics and distance based features\n- Calculating feature statistics by feature grup one another\n- Feature derived from neighbourhood analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"rows = [[4,6,165.977125, 'Bottom_right'],[4,6,34.5395640, 'Bottom_right'],[4,6,29.1963786, 'Bottom_left'],[4,6,79.4373729, 'Bottom_left'],[4,6,290.534595, 'Bottom_right'],\n       [4,6,314.412660, 'Bottom_right'],[4,6,138.9007369, 'Bottom_right'],[4,6,107.4711914, 'Bottom_right'],[4,6,242.1089786, 'Bottom_left'],[4,7,27.16719836, 'Bottom_left'],\n       [4,7,413.5421978, 'Bottom_right']]\n\ndata = pd.DataFrame(rows, columns = ['User_id','Page_id','Ad_price','Ad_position'])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encode the ad_position and fit some classifier and got good score, also it could take into account all the hidden relation between variable. But no matter how good it is, it still treats all data points independently. This is why feature engineering comes into play."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Useful features\nrows = [[4,6,165.977125, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],[4,6,34.5395640, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],\n        [4,6,29.1963786, 'Bottom_left',476.63772, 73.711548,'Bottom_left'],[4,6,79.4373729, 'Bottom_left',476.63772, 73.711548,'Bottom_left'],\n        [4,6,290.534595, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],[4,6,314.412660, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],\n        [4,6,138.9007369, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],[4,6,107.4711914, 'Bottom_right',476.63772, 73.711548,'Bottom_left'],\n        [4,6,242.1089786, 'Bottom_left',476.63772, 73.711548,'Bottom_left'],[4,7,27.16719836, 'Bottom_left',121.54219, 35.465202,'Bottom_left'],\n       [4,7,413.5421978, 'Bottom_right',121.54219, 35.465202,'Bottom_left']]\n\ndata = pd.DataFrame(rows, columns = ['User_id','Page_id','Ad_price','Ad_position', 'Max_price','min_price','Min_price_position'])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is easy to generate feature for overcome such problem. Above table it shows the useful features !\n\n```python\n    # Implementation\n    gb = df.groupby(['user_id','page_id'],as_index=False).agg({'ad_price':{'max_price':np.max,'min_price':np.min}})\n    gb.columns = ['user_id','page_id','min_price','max_price']\n    df = pd.merge(df,gb,how='left',on=['user_id','page_id'])```\n    \n    \n    \n### More features\nThe main ide is to give new information\n\n- How many pages user visited\n- Standard deviation of prices\n- Most visited page\n- Many,many more\n\n\n### Neighbors\nFinding nearest neighbors\n- Explicit group is not needed\n- More flexible \n-  Much harder to implement\n\n\n1. Number of houses in 500m,1000m,...\n2. Average price per square meter in 500m, 1000m,...\n3. Number of school/supermarkets/parking lots in 500m, 1000m,...\n4. Distance to closest subway station"},{"metadata":{},"cell_type":"markdown","source":"### KNN features in Springleaf\n- Calculate various features from those 2000 neighbors\n- Mean target of nearest 5,10,15,500,2000 neighbors\n- Mean distance to 10 closest neighbors\n- Mean distance to 10 closest neighbors with target 1\n- Mean distance to 10 closest neighbors with target 0"},{"metadata":{},"cell_type":"markdown","source":"## Matrix Factorizations --> for Feature Extraction\nUse for text data sets !\n<t> This is very close to the linear model (NMF, PCA)"},{"metadata":{},"cell_type":"markdown","source":"## Feature Interactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [['auto_part','game_news',0],['music_tickets','music_news',1],['mobile_phones','auto_blog',0]]\ndf = pd.DataFrame(rows, columns = ['category_ad','category_site','is_clicked'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combine the categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [['auto_part|game_news',0],['music_tickets|music_news',1],['mobile_phones|auto_blog',0]]\ndf2 = pd.DataFrame(rows, columns=['ad_site','is_clicked'])\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"f1, f2 --> join it to become f_join, then one hot encoding it !\n<t> Similar ideas can be applied to numeric feature --> multiply f1,f2, then we can get the interaction\n    \n### Frequent operations for feature interaction\n- Multiplication\n- Sum\n- Diff\n- Division\n\nThis will make easier for model to learn, but prone to overfit\n\n\n### Practical notes\n- we have a lot of possible interactions - NxN for N features.\n    - Even more if use several types in interactions\n    \n- Need to reduce its number\n    - Dimensionality Reduction\n    - Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Construct all pairwise feature interactions (sums,diffs,dots,divisions)\n1. Fit random forest over them\n2. Get feature importances\n3. Select a few most important\n\nAdded it to original dataset !"},{"metadata":{},"cell_type":"markdown","source":"### Extract features from DT !\nHOW??\n\n<t> The index of object leaf can be use as value for new categoric feature\n    \n#### How to use it ?\n```python\n# In sklearn    \ntree_model.apply()\n    \n# In xgboost\nbooster.predict(pred_leaf=True)```"},{"metadata":{},"cell_type":"markdown","source":"## t-SNE\nVisualing data for generated features --> THIS IS GREAT TOOL FOR VISUALIZATION\n<T> Non-linear model for dimensionality reduction"},{"metadata":{},"cell_type":"markdown","source":"Manifold learning methods !! --> Tries to project from high dimensional space into small dimensional place\n\n#### Interpretation of tSNE ??\ntSNE results strongly depends on its parameter (e.g. Perplexity)\n\n#### How to Use t-SNE effectively --> can obtain new feature from t-SNE plot\nMartin Wattenberg, https://distill.pub/2016/misread-tsne/"},{"metadata":{},"cell_type":"markdown","source":"### Practical note\n1. Result heavily depends on hpyerparameters (perplexity)\n    - Good practice is to use several projections with different perplexities (5-100)\n    \n2. Due to stochastic nature, tSNE provides different projections even for the same data\\hyperparameters\n    - Train and test should be projected together\n    \n3. tSNE runs for a long time with a big number of features\n    - it is common to do dimensionality reduction before projection"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}